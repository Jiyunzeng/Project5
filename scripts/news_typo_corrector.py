# scripts/news_typo_corrector.py (ìµœì í™” ìµœì¢… ë²„ì „)
import os
import requests
from typing import List, Dict, Any, Optional
from dotenv import load_dotenv
from pymongo import MongoClient
from functools import lru_cache

# âš¡ python-Levenshtein ì„¤ì¹˜ í•„ìš”: pip install python-Levenshtein
try:
    from Levenshtein import distance as lev_dist
    FAST_LEVENSHTEIN = True
except ImportError:
    FAST_LEVENSHTEIN = False
    print("âš ï¸ python-Levenshtein ì—†ìŒ. ìˆœìˆ˜ Python ì‚¬ìš© (ëŠë¦¼)")

load_dotenv()

MONGO_URI = os.getenv("MONGO_URI")
PPLX_API_KEY = os.getenv("PERPLEXITY_API_KEY")

client = MongoClient(MONGO_URI)
db = client["stock"]
news_terms = db["news_terms"]

# ğŸ§  ì¸ë±ìŠ¤ ìƒì„± í™•ì¸ (ìµœì´ˆ 1íšŒ)
def ensure_indexes():
    news_terms.create_index([("term", 1), ("freq", -1)])
    news_terms.create_index("term")
    print("âœ… MongoDB ì¸ë±ìŠ¤ í™•ì¸/ìƒì„± ì™„ë£Œ")
ensure_indexes()


# ---------------------------------------
# âš¡ ì´ˆê³ ì† Levenshtein (C í™•ì¥ or Python)
# ---------------------------------------
def levenshtein(a: str, b: str) -> int:
    if FAST_LEVENSHTEIN:
        return lev_dist(a, b)
    # ê¸°ì¡´ ìˆœìˆ˜ Python fallback
    dp = [[i + j if i * j == 0 else 0 for j in range(len(b) + 1)] for i in range(len(a) + 1)]
    for i in range(1, len(a) + 1):
        for j in range(1, len(b) + 1):
            dp[i][j] = min(
                dp[i - 1][j] + 1,
                dp[i][j - 1] + 1,
                dp[i - 1][j - 1] + (0 if a[i - 1] == b[j - 1] else 1)
            )
    return dp[-1][-1]


# ---------------------------------------
# ğŸš€ MongoDB Aggregation ìµœì í™” (30k â†’ 100ê°œ)
# ---------------------------------------
def suggest_news_terms_improved(q: str, limit: int = 10) -> List[Dict[str, Any]]:
    """Aggregation + C-Levenshteinìœ¼ë¡œ 50ë°° ë¹¨ë¼ì§"""
    q = q.strip()
    if len(q) < 2:
        return []
    
    q_len = len(q)
    first_char = q[0].lower()
    
    # Aggregation Pipeline: 30k â†’ 100ê°œë¡œ 300ë°° ì¶•ì†Œ
    pipeline = [
        {
            "$match": {
                "term": {
                    "$regex": f"^{first_char}",  # ì²« ê¸€ì ì •í™• ì¼ì¹˜
                    "$options": "i"
                },
                "$expr": {
                    "$and": [
                        {"$gte": [{"$strLenCP": "$term"}, q_len-4]},
                        {"$lte": [{"$strLenCP": "$term"}, q_len+4]}
                    ]
                },
                "$or": [
                    {"freq": {"$gte": 50}},  # freq 50 ì´ìƒë§Œ
                    {"freq": {"$exists": False}}
                ]
            }
        },
        {"$sort": {"freq": -1}},
        {"$limit": 100},  # í•µì‹¬: í›„ë³´ 100ê°œë¡œ ì œí•œ
        {"$project": {"term": 1, "freq": 1, "top_category": 1}}
    ]
    
    candidates = []
    for doc in news_terms.aggregate(pipeline):
        term = doc.get("term", "")
        if not term or len(term) < 2:
            continue
            
        dist = levenshtein(q, term)
        if dist <= 3:
            score = 1.0 / (dist + 1) + (doc.get("freq", 0) / 10000.0)
            candidates.append({
                "term": term,
                "freq": doc.get("freq", 0),
                "top_category": doc.get("top_category"),
                "dist": dist,
                "score": score
            })
    
    # dist ìš°ì„  + freq ë³µí•© ì •ë ¬
    candidates.sort(key=lambda x: (x["dist"], -x["freq"], -x["score"]))
    return candidates[:limit]


# ---------------------------------------
# ğŸ¤– LLM (ìºì‹± + íƒ€ì„ì•„ì›ƒ)
# ---------------------------------------
@lru_cache(maxsize=128)
def llm_correct_term(original: str) -> str:
    if len(original) < 2:
        return original
        
    url = "https://api.perplexity.ai/chat/completions"
    headers = {
        "Authorization": f"Bearer {PPLX_API_KEY}",
        "Content-Type": "application/json",
    }
    
    content = f"ê²½ì œÂ·ì£¼ì‹Â·ë‰´ìŠ¤ ìš©ì–´ ì˜¤íƒ€ êµì •. '{original}' â†’ ì˜¬ë°”ë¥¸ ë‹¨ì–´ í•˜ë‚˜ë§Œ ì¶œë ¥."

    data = {
        "model": "llama-3.1-sonar-small-128k-online",
        "messages": [{"role": "user", "content": content}],
        "max_tokens": 10,
        "temperature": 0.0,
    }

    try:
        res = requests.post(url, headers=headers, json=data, timeout=3).json()
        return res["choices"][0]["message"]["content"].strip().split()[0]
    except Exception:
        return original


# ---------------------------------------
# ğŸ§  ë©”ì¸ ë¡œì§ (ìºì‹± ì ìš©)
# ---------------------------------------
@lru_cache(maxsize=1000)  # 1000ê°œ ì¿¼ë¦¬ ìºì‹±
def best_news_correction(q: str) -> Dict[str, Any]:
    q = (q or "").strip()

    if len(q) < 2:
        return {"original": q, "corrected": q, "score": 0.0, "source": "too_short"}

    # 1) exact match (ê°€ì¥ ë¹ ë¦„)
    exact_doc = news_terms.find_one({"term": q})
    if exact_doc:
        return {
            "original": q, "corrected": q, "score": 100.0,
            "freq": exact_doc.get("freq", 0),
            "top_category": exact_doc.get("top_category"),
            "is_exact": True, "source": "mongo_exact"
        }

    # 2) ì´ˆê³ ì† Aggregation ê²€ìƒ‰
    cands = suggest_news_terms_improved(q, 10)
    
    if cands:
        best_cand = cands[0]
        best_term = best_cand["term"]
        doc = news_terms.find_one({"term": best_term})
        
        return {
            "original": q, "corrected": best_term,
            "score": 90.0 - best_cand["dist"] * 8,
            "freq": doc.get("freq", best_cand["freq"]) if doc else best_cand["freq"],
            "top_category": doc.get("top_category") if doc else best_cand.get("top_category"),
            "is_exact": False, "source": "aggregation_search"
        }

    # 3) LLM fallback (ìµœí›„ ìˆ˜ë‹¨)
    corrected = llm_correct_term(q)
    return {
        "original": q, "corrected": corrected, "score": 50.0,
        "freq": 0, "top_category": None,
        "is_exact": (corrected == q), "source": "llm_fallback"
    }


# ---------------------------------------
# ğŸ§ª ê³ ì† í…ŒìŠ¤íŠ¸
# ---------------------------------------
def test_correction_speed():
    import time
    test_queries = [
        "ì‚¼ì„±ì €", "íˆ¬ìì", "ê¸ˆìœ ", "ì¸ë„ë„¤ì‹œ", "ë£¨í”¼ì•„",
        "ì• í”Œ", "í…ŒìŠ¬ëŸ¬", "í…ŒìŠ¬ë¼", "ë¹„íŠ¸ì½”ì¸", "ê³µë§¤ë„","ë”¸ê¸”","ìƒ´ì„±"
    ]
    
    print("âš¡ ìµœì í™” ì„±ëŠ¥ í…ŒìŠ¤íŠ¸")
    print("=" * 60)
    
    total_time = 0
    for q in test_queries:
        start = time.time()
        result = best_news_correction(q)
        elapsed = time.time() - start
        
        total_time += elapsed
        status = "âœ…" if result["is_exact"] else "ğŸ”§"
        print(f"{status} '{q}' â†’ '{result['corrected']}' [{result['source']}] {elapsed*1000:.0f}ms")
    
    print(f"\nğŸ¯ í‰ê·  {total_time/len(test_queries)*1000:.0f}ms (ëª©í‘œ: <50ms)")

if __name__ == "__main__":
    test_correction_speed()
