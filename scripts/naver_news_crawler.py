import asyncio
import aiohttp
from bs4 import BeautifulSoup
from datetime import datetime
import os

from pymongo.mongo_client import MongoClient
from pymongo.server_api import ServerApi

# -------------------------
# MongoDB ì—°ê²°
# -------------------------
MONGO_URI = os.environ.get("MONGO_URI")

# ë¡œì»¬ í…ŒìŠ¤íŠ¸í•  ë•Œë§Œ ì•„ë˜ ì£¼ì„ í’€ì–´ì„œ ì‚¬ìš©í•˜ì„¸ìš”
# if not MONGO_URI:
#     MONGO_URI = "mongodb+srv://..." 

if not MONGO_URI:
    raise RuntimeError("MONGO_URI not set in crawler")

client = MongoClient(MONGO_URI, server_api=ServerApi("1"))
db = client["stock"]
collection = db["news_crawling"]

# -------------------------
# ë‰´ìŠ¤ ì¹´í…Œê³ ë¦¬ë³„ URL
# -------------------------
CATEGORY_URLS = {
    "ê¸ˆìœµ": "https://news.naver.com/breakingnews/section/101/259",
    "ì¦ê¶Œ": "https://news.naver.com/breakingnews/section/101/258",
    "ì‚°ì—…/ì¬ê³„": "https://news.naver.com/breakingnews/section/101/261",
    "ì¤‘ê¸°/ë²¤ì²˜": "https://news.naver.com/breakingnews/section/101/771",
    "ê¸€ë¡œë²Œ ê²½ì œ": "https://news.naver.com/breakingnews/section/101/260",
    "ìƒí™œê²½ì œ": "https://news.naver.com/breakingnews/section/101/310",
    "ê²½ì œ ì¼ë°˜": "https://news.naver.com/breakingnews/section/101/263",
}

HEADERS = {"User-Agent": "Mozilla/5.0"}

# -------------------------
# ë¡œê·¸ ì¶œë ¥
# -------------------------
def log(msg):
    print(f"[{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}] {msg}")

# -------------------------
# URL ë³€í™˜
# -------------------------
def to_pc_url(link):
    if "m.news.naver.com" in link:
        return link.replace("m.news.naver.com", "n.news.naver.com")
    return link

# -------------------------
# ë‰´ìŠ¤ ìƒì„¸ í¬ë¡¤ë§
# -------------------------
async def fetch_news_detail(session, link):
    link = to_pc_url(link)
    author = content = media = mediaLogo = image_url = pubDate = ""

    try:
        headers = HEADERS.copy()
        headers.update(
            {
                "Referer": "https://news.naver.com/",
                "Accept-Language": "ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7",
            }
        )

        async with session.get(link, headers=headers, timeout=15) as resp:
            html = await resp.text()
            soup = BeautifulSoup(html, "lxml")

            # ì‘ì„±ì
            author_tag = soup.select_one(
                ".byline span, .byline, .article_info, .writer"
            )
            if author_tag:
                author = author_tag.get_text(strip=True)

            # ë³¸ë¬¸
            content_tag = (
                soup.select_one("#articleBodyContents")
                or soup.select_one("#dic_area")
                or soup.select_one(".news_end")
                or soup.select_one(".article_body")
            )
            if content_tag:
                for s in content_tag.select(
                    "script, style, .ad, .link_area, iframe"
                ):
                    s.decompose()
                content = content_tag.get_text(separator="\n").strip()

            # ì–¸ë¡ ì‚¬
            meta_author = soup.select_one(
                "meta[property='og:article:author'], meta[name='author']"
            )
            if meta_author and meta_author.has_attr("content"):
                media = meta_author["content"].strip()

            # ëŒ€í‘œ ì´ë¯¸ì§€
            meta_image = soup.select_one("meta[property='og:image']")
            if meta_image and meta_image.has_attr("content"):
                image_url = meta_image["content"].strip()

            # ì‘ì„±ì¼
            meta_date = soup.select_one(
                'meta[property="article:published_time"]'
            )
            if meta_date and meta_date.has_attr("content"):
                pubDate = meta_date["content"].strip()
            else:
                date_tag = soup.select_one('span._ARTICLE_DATE_TIME')
                if date_tag and date_tag.has_attr("data-date-time"):
                    pubDate = date_tag["data-date-time"].strip()

            # ì–¸ë¡ ì‚¬ ë¡œê³ 
            def first_url_from_srcset(s):
                if not s:
                    return ""
                parts = s.split(",")
                first = parts[0].strip().split(" ")[0]
                return first

            logo_tag = soup.select_one("img.media_end_head_top_logo_img")
            if logo_tag:
                for a in (
                    "src",
                    "data-src",
                    "data-original",
                    "data-lazy-src",
                    "data-srcset",
                    "srcset",
                ):
                    if logo_tag.has_attr(a):
                        val = logo_tag.get(a, "").strip()
                        if a in ("srcset", "data-srcset"):
                            val = first_url_from_srcset(val)
                        if val:
                            mediaLogo = val
                            break

            if not mediaLogo:
                pc_logo = soup.select_one(".media_end_head_top_logo img")
                if pc_logo:
                    for a in ("src", "data-src", "srcset"):
                        if pc_logo.has_attr(a):
                            val = pc_logo.get(a, "").strip()
                            if a == "srcset":
                                val = first_url_from_srcset(val)
                            if val:
                                mediaLogo = val
                                break

            if not media:
                meta_site = soup.select_one(
                    "meta[property='og:site_name']"
                )
                if meta_site and meta_site.has_attr("content"):
                    media = meta_site["content"].strip()
            if media and media.endswith("| ë„¤ì´ë²„"):
                media = media.replace("| ë„¤ì´ë²„", "").strip()

    except Exception as e:
        log(f"âš  ë‰´ìŠ¤ ìƒì„¸ í¬ë¡¤ë§ ì‹¤íŒ¨: {link} / Error: {e}")

    return author, content, media, mediaLogo, image_url, pubDate

# -------------------------
# ë‰´ìŠ¤ ë¦¬ìŠ¤íŠ¸ í¬ë¡¤ë§
# -------------------------
async def fetch_news_list(session, url, max_items=1000):
    news_list = []
    try:
        async with session.get(url, headers=HEADERS, timeout=10) as resp:
            html = await resp.text()
            soup = BeautifulSoup(html, "lxml")
            items = soup.select("a.sa_text_title")

            for i, a in enumerate(items):
                if i >= max_items:
                    break
                href = a["href"]
                if href.startswith("/"):
                    href = "https://news.naver.com" + href
                title = a.get_text(strip=True)
                news_list.append({"link": href, "title": title})
    except Exception as e:
        log(f"âš  ë‰´ìŠ¤ ë¦¬ìŠ¤íŠ¸ í¬ë¡¤ë§ ì‹¤íŒ¨: {url} / Error: {e}")
    return news_list

# -------------------------
# ì¹´í…Œê³ ë¦¬ë³„ í¬ë¡¤ë§
# -------------------------
async def crawl_category(session, category, url):
    news_list = await fetch_news_list(session, url)
    tasks = []
    valid_news = []

    for news in news_list:
        if collection.find_one({"link": news["link"]}):
            log(f"[SKIP] ì´ë¯¸ ì €ì¥ë¨: {news['title']}")
            continue

        tasks.append(fetch_news_detail(session, news["link"]))
        valid_news.append(news)

        # ì„ì‹œ ë¬¸ì„œ ì‚½ì… (ìƒì„¸ í¬ë¡¤ í›„ í’ˆì§ˆê²€ì‚¬ì—ì„œ ê±¸ëŸ¬ì§ˆ ìˆ˜ ìˆìŒ)
        collection.update_one(
            {"link": news["link"]},
            {
                "$setOnInsert": {
                    "title": news["title"],
                    "link": news["link"],
                    "category": category,
                    "author": "",
                    "content": "",
                    "media": "",
                    "mediaLogo": "",
                    "image_url": "",
                    "pubDate": "",
                }
            },
            upsert=True,
        )

    results = await asyncio.gather(*tasks)

    for (author, content, media, mediaLogo, image_url, pubDate), news in zip(
        results, valid_news
    ):
        has_title = bool(news.get("title", "").strip())
        has_content = bool(content and content.strip())
        has_media = bool(media and media.strip())
        has_date = bool(pubDate and pubDate.strip())

        # ì œëª©ì´ ì—†ê±°ë‚˜, (ë³¸ë¬¸ë„ ì—†ê³  ì–¸ë¡ ì‚¬/ë‚ ì§œë„ ì—†ìœ¼ë©´) ì‚­ì œ
        if not has_title or (not has_content and not (has_media and has_date)):
            log(f"[DROP] ë‚´ìš© ë¶€ì¡±ìœ¼ë¡œ ì‚­ì œ: {news['title']}")
            collection.delete_one({"link": news["link"]})
            continue

        # pubDateê°€ ë¹„ì–´ ìˆìœ¼ë©´ ë‚ ì§œ ì—†ëŠ” ê¸°ì‚¬ë¼ì„œ ì œê±°
        if not has_date:
            log(f"[DROP] ë‚ ì§œ ì—†ìŒìœ¼ë¡œ ì‚­ì œ: {news['title']}")
            collection.delete_one({"link": news["link"]})
            continue

        collection.update_one(
            {"link": news["link"]},
            {
                "$set": {
                    "author": author,
                    "content": content,
                    "media": media,
                    "mediaLogo": mediaLogo,
                    "image_url": image_url,
                    "pubDate": pubDate,
                }
            },
        )

    log(f"âœ… {category} ë‰´ìŠ¤ í¬ë¡¤ë§ ì™„ë£Œ. ì´ ì €ì¥: {len(valid_news)}ê±´")

# -------------------------
# [ìˆ˜ì •ë¨] ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜
# ì´ë¦„ ë³€ê²½: main -> task_korea_crawling
# -------------------------
async def task_korea_crawling():
    async with aiohttp.ClientSession() as session:
        for category, url in CATEGORY_URLS.items():
            log(f"=== ğŸ‡°ğŸ‡· êµ­ë‚´ ë‰´ìŠ¤ í¬ë¡¤ë§ ì‹œì‘: {category} ===")
            await crawl_category(session, category, url)
        log("ğŸ‰ êµ­ë‚´ ë‰´ìŠ¤ í¬ë¡¤ë§ ì „ì²´ ì™„ë£Œ!")

# ì›ë˜ ìˆë˜ ë¬´í•œë£¨í”„(periodic_crawl)ì™€ ì‹¤í–‰ë¶€(__name__)ëŠ” ì‚­ì œí–ˆìŠµë‹ˆë‹¤.
# app.pyì—ì„œ task_korea_crawling í•¨ìˆ˜ë§Œ importí•´ì„œ ì‚¬ìš©í•©ë‹ˆë‹¤.
