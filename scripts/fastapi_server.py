# fastapi_server.py - ê¸°ì¡´ íŒŒì¼ì— News Terms Streamer ì¶”ê°€
from typing import List, Dict, Any
import time
import os
import re
import asyncio
import pickle
import hashlib
import json
from collections import Counter, defaultdict
from typing import Set

from dotenv import load_dotenv
from fastapi import FastAPI, BackgroundTasks
from pydantic import BaseModel
import uvicorn
from fastapi.middleware.cors import CORSMiddleware
from motor.motor_asyncio import AsyncIOMotorClient
from pymongo import UpdateOne
from konlpy.tag import Okt
from contextlib import asynccontextmanager

import redis

# ---------- ê¸°ì¡´ ëª¨ë“ˆë“¤ ----------
from nlp_search import enhanced_tokenize
from tfidf_rank_lib import rank_with_tfidf
from ime_converter import to_hangul
from chat_summary_lib import build_summary, ChatSummaryResponse
from news_typo_corrector import best_news_correction

# ---------- í™˜ê²½ë³€ìˆ˜ ----------
load_dotenv()

# ---------- MongoDB ì„¤ì • ----------
MONGO_URI = os.getenv("MONGO_URI")
CATEGORIES = ["ê¸ˆìœµ","ì¦ê¶Œ","ì‚°ì—…/ì¬ê³„","ì¤‘ê¸°/ë²¤ì²˜","ê¸€ë¡œë²Œ ê²½ì œ","ìƒí™œê²½ì œ","ê²½ì œ ì¼ë°˜"]

# ì „ì—­ ë³€ìˆ˜ë“¤ (News Termsìš©)
mongo_client = None
news_collection = None
news_terms_collection = None
stopwords: Set[str] = None
okt = Okt()
news_terms_task = None

# ---------- FastAPI ----------
app = FastAPI(title="Project5 AI Search API", version="2.4")

origins = [
    "http://localhost:5173",
    "http://127.0.0.1:5173",
    "http://localhost:3000",
]

app.add_middleware(
    CORSMiddleware,
    allow_origins=origins,
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ---------- Redis ----------
REDIS_HOST = os.getenv("REDIS_HOST", "localhost")
REDIS_PORT = int(os.getenv("REDIS_PORT", 6379))
CACHE_TTL = 300

redis_client = redis.Redis(
    host=REDIS_HOST,
    port=REDIS_PORT,
    db=0,
    decode_responses=False
)

# ---------- News Terms í—¬í¼ í•¨ìˆ˜ë“¤ ----------
def load_stopwords() -> Set[str]:
    """ë¶ˆìš©ì–´ ë¡œë“œ"""
    file_path = "scripts/stopwords_kor.txt"  # scripts í´ë”ì— ë§ê²Œ ì¡°ì •
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            stopwords = set()
            for line in f:
                word = line.strip()
                if word and re.match('^[ê°€-í£]+$', word):
                    stopwords.add(word)
        print(f"âœ… ë¶ˆìš©ì–´ ë¡œë“œ ì™„ë£Œ: {len(stopwords)}ê°œ")
        return stopwords
    except Exception as e:
        print(f"âŒ ë¶ˆìš©ì–´ ë¡œë“œ ì‹¤íŒ¨: {e}")
        return {"ìˆë‹¤", "ìˆëŠ”", "í•˜ë‹¤", "ë˜ëŠ”", "ë°í˜”ë‹¤", "ê¸°ì", "ë“±", "í†µí•´", "ìœ„í•´"}

def extract_nouns_kor(text: str, stopwords: Set[str]) -> list[str]:
    """Okt + ë¶ˆìš©ì–´ ì œê±°ë¡œ ê³ í’ˆì§ˆ ëª…ì‚¬ ì¶”ì¶œ"""
    if not text or len(text.strip()) < 2:
        return []
    
    try:
        nouns = okt.nouns(text)
        filtered_nouns = [
            noun for noun in nouns 
            if (len(noun) >= 2 and 
                re.match('^[ê°€-í£]+$', noun) and 
                noun not in stopwords)
        ]
        return filtered_nouns
    except Exception as e:
        print(f"âš ï¸ í˜•íƒœì†Œ ë¶„ì„ ì‹¤íŒ¨: {e}")
        words = re.findall(r'[ê°€-í£]{2,}', text)
        return [w for w in words if w not in stopwords]

async def process_single_doc(doc: Dict[str, Any]):
    """ë‹¨ì¼ ë¬¸ì„œ ì²˜ë¦¬ í›„ bulk upsert"""
    title = doc.get("title", "")
    content = doc.get("content", "")
    cat = doc.get("category", "")
    
    if cat not in CATEGORIES:
        return
        
    text = f"{title} {content}".strip()
    nouns = extract_nouns_kor(text, stopwords)
    
    if not nouns:
        return
    
    # Bulk update operations
    bulk_ops = []
    for noun in nouns:
        # ê¸°ì¡´ ì¹´í…Œê³ ë¦¬ freq í™•ì¸ í›„ ì¦ê°€
        existing = await news_terms_collection.find_one({"term": noun})
        current_cat_freq = existing.get("categories", {}).get(cat, 0) if existing else 0
        
        bulk_ops.append(
            UpdateOne(
                {"term": noun},
                {
                    "$inc": {"freq": 1},
                    "$set": {
                        f"categories.{cat}": current_cat_freq + 1
                    },
                    "$setOnInsert": {
                        "top_category": None,
                        "source": "news_crawling"
                    }
                },
                upsert=True
            )
        )
    
    if bulk_ops:
        await news_terms_collection.bulk_write(bulk_ops, ordered=False)
        print(f"âœ… [{cat}] {len(nouns)}ê°œ term ì—…ë°ì´íŠ¸: {nouns[:3]}...")

async def run_news_terms_stream():
    """MongoDB change streamìœ¼ë¡œ ì‹¤ì‹œê°„ ì²˜ë¦¬"""
    print("ğŸ” News Terms Change Stream ì‹œì‘...")
    
    pipeline = [
        {"$match": {
            "operationType": "insert",
            "fullDocument.category": {"$in": CATEGORIES}
        }}
    ]
    
    while True:
        try:
            async with news_collection.watch(pipeline) as stream:
                async for change in stream:
                    doc = change["fullDocument"]
                    await process_single_doc(doc)
        except Exception as e:
            print(f"âŒ Change Stream ì˜¤ë¥˜: {e}")
            await asyncio.sleep(5)

# ---------- FastAPI ìƒëª…ì£¼ê¸° ----------
@asynccontextmanager
async def lifespan(app: FastAPI):
    """ì„œë²„ ì‹œì‘/ì¢…ë£Œ ì‹œ News Terms ì›Œì»¤ ê´€ë¦¬"""
    global mongo_client, news_collection, news_terms_collection, stopwords, news_terms_task
    
    # Startup
    print("ğŸš€ MongoDB ì—°ê²° ì¤‘...")
    mongo_client = AsyncIOMotorClient(MONGO_URI)
    db = mongo_client["stock"]
    news_collection = db["news_crawling"]
    news_terms_collection = db["news_terms"]
    stopwords = load_stopwords()
    
    # News Terms ì›Œì»¤ ì‹œì‘
    news_terms_task = asyncio.create_task(run_news_terms_stream())
    print("âœ… News Terms Streamer í™œì„±í™”ë¨")
    
    yield
    
    # Shutdown
    print("ğŸ›‘ News Terms Streamer ì¢…ë£Œ ì¤‘...")
    if news_terms_task:
        news_terms_task.cancel()
        try:
            await news_terms_task
        except asyncio.CancelledError:
            pass
    if mongo_client:
        mongo_client.close()

app.router.lifespan_context = lifespan

# ---------- ê¸°ì¡´ Cache Utils (ë³€ê²½ì—†ìŒ) ----------
def get_cache(key: str):
    try:
        data = redis_client.get(key)
        if data:
            return pickle.loads(data)
    except Exception:
        pass
    return None

def set_cache(key: str, value: Any, ttl: int = CACHE_TTL):
    try:
        redis_client.setex(key, ttl, pickle.dumps(value))
    except Exception:
        pass

def make_docs_hash(documents: List[Dict[str, Any]]) -> str:
    ids = [str(d["id"]) for d in documents if "id" in d and d["id"] is not None]
    ids.sort()
    return hashlib.md5(",".join(ids).encode()).hexdigest()

# ---------- ê¸°ì¡´ APIë“¤ (ë³€ê²½ì—†ìŒ) ----------
class NlpRequest(BaseModel):
    query: str

@app.post("/nlp-analyze")
def nlp_analyze(req: NlpRequest):
    tokens = enhanced_tokenize(req.query)
    return {"tokens": tokens, "count": len(tokens)}

class TfidfRequest(BaseModel):
    query: str
    documents: List[Dict[str, Any]]

@app.post("/tfidf-rank")
def tfidf_rank(req: TfidfRequest):
    original_query = req.query.strip()
    correction = best_news_correction(original_query)
    corrected_query = correction["corrected"].strip().lower()
    
    docs_hash = make_docs_hash(req.documents)
    cache_key = f"tfidf:{corrected_query}:{docs_hash}"
    
    cached = get_cache(cache_key)
    if cached:
        cached["cached"] = True
        print("âš¡ TF-IDF cache HIT")
        return cached
    
    print("ğŸ¢ TF-IDF cache MISS - ê³„ì‚° ì¤‘...")
    ranked = rank_with_tfidf(corrected_query, req.documents)
    
    result = {
        "original_query": original_query,
        "corrected_query": corrected_query,
        "correction": correction,
        "ranked_docs": ranked,
        "total": len(ranked),
        "cached": False
    }
    
    set_cache(cache_key, result)
    print(f"âœ… TF-IDF ìºì‹œ ì €ì¥: {cache_key[:30]}...")
    return result

@app.get("/ime-convert")
def ime_convert(q: str):
    return {"original": q, "converted": to_hangul(q or "")}

@app.get("/news-search-correction")
def news_search_correction(q: str):
    original = (q or "").strip()
    ime_q = to_hangul(original)
    base_q = ime_q or original
    cache_key = f"news_corr:{base_q}"

    cached = get_cache(cache_key)
    if cached:
        cached["cached"] = True
        return cached

    news_corr = best_news_correction(base_q)
    result = {
        "original": original,
        "ime_converted": ime_q,
        "news": news_corr,
        "cached": False
    }
    set_cache(cache_key, result)
    return result

class SummaryRequest(BaseModel):
    query: str

@app.post("/chat-summary", response_model=ChatSummaryResponse)
def chat_summary(req: SummaryRequest):
    return build_summary(req.query)

# ---------- ìƒˆë¡œìš´ News Terms APIë“¤ ----------
@app.get("/news-terms/health")
async def news_terms_health():
    """News Terms ìƒíƒœ í™•ì¸"""
    total_terms = await news_terms_collection.count_documents({})
    return {
        "status": "ğŸŸ¢ running",
        "total_terms": total_terms,
        "categories": CATEGORIES
    }

@app.get("/news-terms/top/{limit}")
async def get_top_terms(limit: int = 50):
    """ìƒìœ„ ìš©ì–´ ì¡°íšŒ"""
    pipeline = [
        {"$addFields": {"total_freq": {"$sum": "$categories"}}},
        {"$sort": {"total_freq": -1}},
        {"$limit": limit},
        {"$project": {
            "term": 1,
            "freq": 1,
            "categories": 1,
            "top_category": 1
        }}
    ]
    cursor = news_terms_collection.aggregate(pipeline)
    terms = await cursor.to_list(length=limit)
    return {"terms": terms, "total": len(terms)}

@app.post("/news-terms/rebuild")
async def rebuild_terms(background_tasks: BackgroundTasks):
    """ì „ì²´ ì¬êµ¬ì¶• (ë°±ê·¸ë¼ìš´ë“œ)"""
    background_tasks.add_task(build_full_terms)
    return {"status": "rebuild started"}

async def build_full_terms():
    """ì´ˆê¸° ì „ì²´ ë°ì´í„° ì¬êµ¬ì¶•"""
    print("ğŸ”„ News Terms ì „ì²´ ì¬êµ¬ì¶• ì‹œì‘...")
    await news_terms_collection.delete_many({})
    
    total_counter = Counter()
    category_counters = defaultdict(Counter)
    
    async for doc in news_collection.find({"category": {"$in": CATEGORIES}}):
        text = f"{doc.get('title', '')} {doc.get('content', '')}".strip()
        nouns = extract_nouns_kor(text, stopwords)
        if nouns:
            total_counter.update(nouns)
            category_counters[doc.get("category", "")].update(nouns)
    
    docs = []
    for term, freq in total_counter.most_common(30000):
        cat_freqs = {cat: int(category_counters[cat][term]) for cat in CATEGORIES if category_counters[cat][term] > 0}
        top_category = max(cat_freqs, key=cat_freqs.get, default=None) if cat_freqs else None
        docs.append({
            "term": term,
            "freq": int(freq),
            "categories": cat_freqs,
            "top_category": top_category,
            "source": "news_crawling",
        })
    
    if docs:
        await news_terms_collection.insert_many(docs)
        print(f"âœ… ì „ì²´ ì¬êµ¬ì¶• ì™„ë£Œ: {len(docs)}ê°œ")

# ---------- ê¸°ì¡´ Health (í™•ì¥) ----------
@app.get("/health")
def health_check():
    return {
        "status": "ğŸŸ¢ healthy",
        "redis": "connected" if redis_client.ping() else "disconnected",
        "news_terms": "enabled",
        "version": "2.4"
    }

# ---------- Run ----------
if __name__ == "__main__":
    print("ğŸš€ Project5 AI Search API v2.4 (News Terms Streamer í¬í•¨)")
    uvicorn.run(
        "fastapi_server:app",
        host="0.0.0.0",
        port=8000,
        reload=True
    )
