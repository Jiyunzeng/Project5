from flask import Flask, jsonify, request
from flask_cors import CORS
from urllib.parse import unquote
from datetime import datetime, timedelta
import threading, time, os, asyncio

from pymongo.mongo_client import MongoClient
from pymongo.server_api import ServerApi

import scripts.naver_news_crawler as crawler

# ğŸ”¹ Redis ì¶”ê°€
import redis
import json

app = Flask(__name__)
CORS(app)  # CORS ì„¤ì •: ì™¸ë¶€ì—ì„œ API ì ‘ê·¼ ê°€ëŠ¥

# ==========================
# MongoDB & Redis ì„¤ì •
# ==========================
MONGO_URI = os.environ.get("MONGO_URI")  # í™˜ê²½ë³€ìˆ˜ë¡œ MongoDB URI ê°€ì ¸ì˜¤ê¸°
if not MONGO_URI:
    raise RuntimeError("MONGO_URI not set in Flask")  # URI ì—†ìœ¼ë©´ ë°”ë¡œ ì˜¤ë¥˜

client = MongoClient(MONGO_URI, server_api=ServerApi("1"))  # MongoDB ì—°ê²°
db = client["stock"]  # stock DB ì„ íƒ
collection = db["news_crawling"]  # news_crawling ì»¬ë ‰ì…˜ ì„ íƒ

REDIS_URL = os.environ.get("REDIS_URL", "redis://localhost:6380/0")  # Redis ì—°ê²° ì£¼ì†Œ
redis_client = redis.from_url(REDIS_URL, decode_responses=True)  # ë¬¸ìì—´ ìë™ ë””ì½”ë”©
CACHE_TTL = 60  # Redis ìºì‹œ ìœ íš¨ ì‹œê°„ (ì´ˆ): 1ë¶„

# ==========================
# pubDate íŒŒì‹± ìœ í‹¸
# ==========================
def _parse_pub_date(value):
    """
    pubDate í•„ë“œë¥¼ datetime ê°ì²´ë¡œ ë³€í™˜.
    - ì´ë¯¸ datetimeì´ë©´ ê·¸ëŒ€ë¡œ ë°˜í™˜
    - ë¬¸ìì—´ì´ë©´ ì—¬ëŸ¬ í¬ë§·(ISO8601, YYYY-MM-DD HH:MM:SS, YYYY-MM-DD)ì„ ì‹œë„í•´ì„œ íŒŒì‹±
    - ëª¨ë‘ ì‹¤íŒ¨í•˜ë©´ None ë°˜í™˜
    """
    if isinstance(value, datetime):
        return value  # ì´ë¯¸ datetimeì´ë©´ ê·¸ëŒ€ë¡œ

    if isinstance(value, str):
        v = value.strip()
        if not v:
            return None  # ë¹ˆ ë¬¸ìì—´ì€ None

        # ISO8601 í¬ë§· ì‹œë„ (2025-01-01T10:00:00Z ë“±)
        try:
            return datetime.fromisoformat(v.replace("Z", "+00:00"))
        except Exception:
            pass

        # YYYY-MM-DD HH:MM:SS, YYYY-MM-DD í¬ë§· ì‹œë„
        for fmt in ("%Y-%m-%d %H:%M:%S", "%Y-%m-%d"):
            try:
                return datetime.strptime(v, fmt)
            except ValueError:
                continue

    return None  # ë³€í™˜ ì‹¤íŒ¨ ì‹œ None

# ==========================
# í•œ ë‹¬ ì§€ë‚œ ê¸°ì‚¬ ì‚­ì œ
# ==========================
def delete_old_news(days: int = 30):
    """
    pubDate ê¸°ì¤€ìœ¼ë¡œ daysì¼ ì§€ë‚œ ê¸°ì‚¬ ì‚­ì œ.
    - MongoDBì—ì„œ pubDate < (í˜„ì¬ì‹œê°„ - days) ì¸ ëª¨ë“  ë¬¸ì„œ ì‚­ì œ
    - ì‚­ì œ ê°œìˆ˜ì™€ ê¸°ì¤€ì¼ì„ ë¡œê·¸ ì¶œë ¥
    """
    threshold = datetime.now() - timedelta(days=days)  # ê¸°ì¤€ ì‹œê° ê³„ì‚°
    try:
        result = collection.delete_many({"pubDate": {"$lt": threshold}})
        print(f"[CLEANUP] {result.deleted_count}ê°œ ì‚­ì œ (ê¸°ì¤€ì¼: {threshold})")
    except Exception as e:
        print(f"[CLEANUP ERROR] ì˜¤ë˜ëœ ë‰´ìŠ¤ ì‚­ì œ ì‹¤íŒ¨: {e}")  # ì˜ˆì™¸ ë°œìƒ ì‹œ ë¡œê·¸

# ==========================
# Mongo ì •ë ¬ + í˜ì´ì§€ë„¤ì´ì…˜
# ==========================
def _sort_and_page(query, page, size, order):
    """
    MongoDBì—ì„œ ì¿¼ë¦¬ ì¡°ê±´ì— ë§ëŠ” ë‰´ìŠ¤ë¥¼ ì •ë ¬, í˜ì´ì§€ë„¤ì´ì…˜í•˜ì—¬ ë°˜í™˜.
    - query: MongoDB ì¡°íšŒ ì¡°ê±´ (dict)
    - page: í˜ì´ì§€ ë²ˆí˜¸ (0ë¶€í„° ì‹œì‘)
    - size: í˜ì´ì§€ë‹¹ ê°œìˆ˜
    - order: ì •ë ¬ ë°©í–¥ ('asc' ë˜ëŠ” 'desc')
    - ë°˜í™˜: (content ë¦¬ìŠ¤íŠ¸, total_pages)
    """
    sort_dir = -1 if order != "asc" else 1  # ë‚´ë¦¼ì°¨ìˆœ: -1, ì˜¤ë¦„ì°¨ìˆœ: 1

    # MongoDB ì¡°íšŒ: ì¡°ê±´ì— ë§ëŠ” ë¬¸ì„œ ì •ë ¬ í›„ ìŠ¤í‚µ/ë¦¬ë°‹ ì ìš©
    cursor = (
        collection.find(query, {"_id": 0})  # _id í•„ë“œ ì œì™¸
        .sort("pubDate", sort_dir)  # pubDate ê¸°ì¤€ ì •ë ¬
        .skip(page * size)  # í˜ì´ì§€ ìŠ¤í‚µ
        .limit(size)  # í˜ì´ì§€ í¬ê¸° ì œí•œ
    )

    content = []
    for news in cursor:
        parsed = _parse_pub_date(news.get("pubDate"))  # pubDate íŒŒì‹±
        if parsed is None:
            continue  # íŒŒì‹± ì‹¤íŒ¨í•˜ë©´ ê±´ë„ˆëœ€
        news["pubDate"] = parsed.strftime("%Y-%m-%d %H:%M:%S")  # í¬ë§· í†µì¼
        content.append(news)

    total_count = collection.count_documents(query)  # ì „ì²´ ë¬¸ì„œ ê°œìˆ˜
    total_pages = (total_count + size - 1) // size  # ì´ í˜ì´ì§€ ìˆ˜ ê³„ì‚°

    return content, total_pages

# ==========================
# Redis ìºì‹œ ìœ í‹¸
# ==========================
def _cache_key(prefix, category, page, size, order):
    """
    ìºì‹œ í‚¤ ìƒì„±: ë™ì¼ ì¡°ê±´ì˜ ìš”ì²­ì— ëŒ€í•´ ê°™ì€ í‚¤ë¥¼ ì‚¬ìš©.
    - prefix: ìºì‹œ êµ¬ë¶„ìš© (ì˜ˆ: 'news')
    - category: ì¹´í…Œê³ ë¦¬ (ì—†ìœ¼ë©´ ë¹ˆ ë¬¸ìì—´)
    - page, size, order: í˜ì´ì§€ë„¤ì´ì…˜ ë° ì •ë ¬ ì •ë³´
    - ë°˜í™˜: ìºì‹œ í‚¤ ë¬¸ìì—´
    """
    cat = category or ""
    return f"{prefix}:cat={cat}:page={page}:size={size}:order={order}"

def get_news_with_cache(prefix, category, page, size, order, query):
    """
    Redis ìºì‹œì—ì„œ ë‰´ìŠ¤ ëª©ë¡ì„ ì¡°íšŒí•˜ê³ , ì—†ìœ¼ë©´ MongoDBì—ì„œ ì¡°íšŒ í›„ ìºì‹œ ì €ì¥.
    - Redis ì¥ì•  ì‹œ ìºì‹œë¥¼ ë¬´ì‹œí•˜ê³  MongoDBì—ì„œ ì§ì ‘ ì¡°íšŒ.
    - ê²°ê³¼: {"content": [...], "number": page, "totalPages": total_pages}
    """
    key = _cache_key(prefix, category, page, size, order)  # ìºì‹œ í‚¤ ìƒì„±

    # 1) ìºì‹œ ì¡°íšŒ
    try:
        cached = redis_client.get(key)
        if cached:
            return json.loads(cached)  # ìºì‹œ ìˆìœ¼ë©´ ë°”ë¡œ ë°˜í™˜
    except Exception:
        cached = None  # Redis ì¥ì•  ì‹œ ìºì‹œ ë¬´ì‹œ

    # 2) Mongo ì¡°íšŒ
    content, total_pages = _sort_and_page(query, page, size, order)
    result = {"content": content, "number": page, "totalPages": total_pages}

    # 3) ìºì‹œì— ì €ì¥
    try:
        redis_client.setex(key, CACHE_TTL, json.dumps(result))  # TTL ì ìš© ì €ì¥
    except Exception:
        pass  # ì €ì¥ ì‹¤íŒ¨ ì‹œ ë¬´ì‹œ

    return result

# ==========================
# Flask ë¼ìš°íŠ¸
# ==========================
@app.route("/")
def index():
    """
    ì„œë²„ ìƒíƒœ í™•ì¸ìš© ë¼ìš°íŠ¸.
    - ì‘ë‹µ: "Flask API is running"
    """
    return "Flask API is running"

@app.route("/news")
def get_news():
    """
    ì¹´í…Œê³ ë¦¬ë³„ ìµœì‹  ë‰´ìŠ¤ ëª©ë¡ API.
    - ì¿¼ë¦¬ íŒŒë¼ë¯¸í„°: category, page, size, order
    - Redis ìºì‹œ ì‚¬ìš© (1ë¶„)
    - ì‘ë‹µ: {"content": [...], "number": page, "totalPages": total_pages}
    """
    category = unquote(request.args.get("category", ""))  # ì¹´í…Œê³ ë¦¬ (URL ë””ì½”ë”©)
    page = int(request.args.get("page", 0))  # í˜ì´ì§€ ë²ˆí˜¸
    size = int(request.args.get("size", 5))  # í˜ì´ì§€ë‹¹ ê°œìˆ˜
    order = request.args.get("order", "desc")  # ì •ë ¬ ë°©í–¥

    query = {"category": category} if category else {}  # ì¹´í…Œê³ ë¦¬ ì¡°ê±´

    result = get_news_with_cache("news", category, page, size, order, query)
    return jsonify(result)

@app.route("/news/search")
def search_news():
    """
    í‚¤ì›Œë“œ ê²€ìƒ‰ + ì¹´í…Œê³ ë¦¬ í•„í„° API.
    - ì¿¼ë¦¬ íŒŒë¼ë¯¸í„°: q, category, page, size, order
    - MongoDBì—ì„œ ì œëª©/ë³¸ë¬¸/ì‘ì„±ì/ì–¸ë¡ ì‚¬ì—ì„œ ê²€ìƒ‰
    - ì‘ë‹µ: {"content": [...], "number": page, "totalPages": total_pages}
    """
    q = request.args.get("q", "").strip()  # ê²€ìƒ‰ì–´
    category = unquote(request.args.get("category", ""))  # ì¹´í…Œê³ ë¦¬
    page = int(request.args.get("page", 0))  # í˜ì´ì§€ ë²ˆí˜¸
    size = int(request.args.get("size", 5))  # í˜ì´ì§€ë‹¹ ê°œìˆ˜
    order = request.args.get("order", "desc")  # ì •ë ¬ ë°©í–¥

    if not q:
        return jsonify({"content": [], "number": 0, "totalPages": 0})  # ê²€ìƒ‰ì–´ ì—†ìœ¼ë©´ ë¹ˆ ê²°ê³¼

    regex = {"$regex": q, "$options": "i"}  # ëŒ€ì†Œë¬¸ì ë¬´ì‹œ ì •ê·œì‹

    or_query = {
        "$or": [
            {"title": regex},  # ì œëª© ê²€ìƒ‰
            {"content": regex},  # ë³¸ë¬¸ ê²€ìƒ‰
            {"author": regex},  # ì‘ì„±ì ê²€ìƒ‰
            {"media": regex},  # ì–¸ë¡ ì‚¬ ê²€ìƒ‰
        ]
    }

    if category:
        query = {"$and": [{"category": category}, or_query]}  # ì¹´í…Œê³ ë¦¬ + ê²€ìƒ‰
    else:
        query = or_query  # ê²€ìƒ‰ë§Œ

    content, total_pages = _sort_and_page(query, page, size, order)
    return jsonify({"content": content, "number": page, "totalPages": total_pages})

# ==========================
# í¬ë¡¤ëŸ¬ ì‹¤í–‰ ìŠ¤ë ˆë“œ
# ==========================
def run_crawler():
    """
    ë°±ê·¸ë¼ìš´ë“œ ìŠ¤ë ˆë“œì—ì„œ ì£¼ê¸°ì ìœ¼ë¡œ í¬ë¡¤ëŸ¬ ì‹¤í–‰ ë° ì˜¤ë˜ëœ ë‰´ìŠ¤ ì •ë¦¬.
    - 1ì‹œê°„ë§ˆë‹¤ crawler.main() ì‹¤í–‰
    - ì‹¤í–‰ í›„ 30ì¼ ì§€ë‚œ ë‰´ìŠ¤ ì‚­ì œ
    - ë¬´í•œ ë£¨í”„
    """
    while True:
        asyncio.run(crawler.task_korea_crawling())  # ë¹„ë™ê¸° í¬ë¡¤ëŸ¬ ì‹¤í–‰
        delete_old_news(30)  # 30ì¼ ì§€ë‚œ ë‰´ìŠ¤ ì‚­ì œ
        time.sleep(3600)  # 1ì‹œê°„ ëŒ€ê¸°

# ==========================
# ì—”íŠ¸ë¦¬ í¬ì¸íŠ¸
# ==========================
if __name__ == "__main__":
    """
    ì„œë²„ ì‹œì‘ ì‹œì :
    - ë°±ê·¸ë¼ìš´ë“œ ìŠ¤ë ˆë“œì—ì„œ run_crawler() ì‹¤í–‰ (ë°ëª¬)
    - Flask ì„œë²„ êµ¬ë™ (PORT í™˜ê²½ë³€ìˆ˜ ë˜ëŠ” ê¸°ë³¸ 8585)
    - debug=False: ìš´ì˜ í™˜ê²½
    """
    threading.Thread(target=run_crawler, daemon=True).start()  # ë°±ê·¸ë¼ìš´ë“œ í¬ë¡¤ëŸ¬
    port = int(os.environ.get("PORT", 8585))  # í¬íŠ¸ ì„¤ì •
    app.run(host="0.0.0.0", port=port, debug=False)  # ì„œë²„ ì‹¤í–‰
