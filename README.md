# StockNews ğŸš€

[![ê°œë°œ ê¸°ê°„](https://img.shields.io/badge/ê°œë°œ%20ê¸°ê°„-2025.12.02%20~%202025.12.16-blue?style=flat-square)]
[![íŒ€ì› ìˆ˜](https://img.shields.io/badge/íŒ€ì›-6ëª…-green?style=flat-square)]

<img src="https://github.com/user-attachments/assets/1231967b-c1d3-4f0e-8ad4-81def337bf63" alt="í”„ë¡œì íŠ¸ ë©”ì¸ ëŒ€ì‹œë³´ë“œ ìŠ¤í¬ë¦°ìƒ·" />

## í”„ë¡œì íŠ¸ ì†Œê°œ

**ì‹¤ì‹œê°„ êµ­ë‚´ ì£¼ì‹ ì‹œì„¸ ëª¨ë‹ˆí„°ë§ ì›¹ ëŒ€ì‹œë³´ë“œ** ğŸ“ˆ

ì´ í”„ë¡œì íŠ¸ëŠ” ì›¹ í¬ë¡¤ë§ ê¸°ìˆ ì„ í™œìš©í•œ ë¡œë´‡ì´ KOSPI/KOSDAQ ì¢…ëª©ì˜ ì‹¤ì‹œê°„ ì‹œì„¸ì™€ êµ­ë‚´Â·í•´ì™¸ ê´€ë ¨ ë‰´ìŠ¤ë¥¼ ìë™ ìˆ˜ì§‘í•˜ê³ ,  
ìˆ˜ì§‘ëœ ë°ì´í„°ë“¤ì„ ê°€ê³µí•˜ì—¬ ì‚¬ìš©ìì—ê²Œ í•œëˆˆì— ë³´ì—¬ì¤ë‹ˆë‹¤.

### ê°œë°œ ê¸°ê°„
2025.12.02 ~ 2025.12.16

### íŒ€ì› ë° ì—­í• 

| ì´ë¦„   | ì—­í•                           | GitHub                                                                 |
|--------|-------------------------------|------------------------------------------------------------------------|
| ì •íƒœê·œ | [íŒ€ì¥] êµ­ë‚´ì£¼ì‹ í¬ë¡¤ë§, ì‹¤ì‹œê°„ ëŒ€ì‹œë³´ë“œ ë“±            | [![GitHub](https://img.shields.io/badge/GitHub-000000?style=flat&logo=github&logoColor=white)](https://github.com/KANASIEL) |
| ì¡°ìŠ¬ë¯¸ | êµ­ë‚´/í•´ì™¸ ë‰´ìŠ¤ í¬ë¡¤ë§, ë‰´ìŠ¤í˜ì´ì§€ ë“±                  | [![GitHub](https://img.shields.io/badge/GitHub-000000?style=flat&logo=github&logoColor=white)](https://github.com/jseulmi) |
| ì„œì›í¬ | êµ­ë‚´/í•´ì™¸ ë‰´ìŠ¤ í¬ë¡¤ë§, ë‰´ìŠ¤í˜ì´ì§€ ë“±                  | [![GitHub](https://img.shields.io/badge/GitHub-000000?style=flat&logo=github&logoColor=white)](https://github.com/wonhui29) |
| êµ¬í˜„ì„œ | ë¡œê·¸ì¸/íšŒì›ê°€ì…, ë‹¤êµ­ì–´UI ë“±                         | [![GitHub](https://img.shields.io/badge/GitHub-000000?style=flat&logo=github&logoColor=white)](https://github.com/guhyeonseo) |
| ì†ì›ì£¼ | ê²€ìƒ‰ì—”ì§„ (í˜•íƒœì†Œ ë¶„ì„ TF-IDFë­í‚¹ ì˜¤íƒ€ë³´ì •), AIìš”ì•½ ë“± | [![GitHub](https://img.shields.io/badge/GitHub-000000?style=flat&logo=github&logoColor=white)](https://github.com/swj6498) |
| ì§€ìœ¤ì • | ê²€ìƒ‰ì—”ì§„ (í˜•íƒœì†Œ ë¶„ì„ TF-IDFë­í‚¹ ì˜¤íƒ€ë³´ì •), ì¸ê¸°ê²€ìƒ‰ì–´ ë“± | [![GitHub](https://img.shields.io/badge/GitHub-000000?style=flat&logo=github&logoColor=white)](https://github.com/Jiyunzeng) |

## ê¸°ìˆ  ìŠ¤íƒ ğŸ› ï¸

| ì¹´í…Œê³ ë¦¬             | ê¸°ìˆ                                                                                                                                  |
|----------------------|--------------------------------------------------------------------------------------------------------------------------------------|
| ìš´ì˜ì²´ì œ             | ![Ubuntu](https://img.shields.io/badge/Ubuntu-E95420?style=flat&logo=ubuntu&logoColor=white)&nbsp;![Windows 11](https://img.shields.io/badge/Windows%2011-0078D6?style=flat&logo=windows11&logoColor=white) |
| ì–¸ì–´                 | ![Java](https://img.shields.io/badge/Java-ED8B00?style=flat&logo=openjdk&logoColor=white)&nbsp;![Python](https://img.shields.io/badge/Python-3776AB?style=flat&logo=python&logoColor=white)&nbsp;![JavaScript](https://img.shields.io/badge/JavaScript-F7DF1E?style=flat&logo=javascript&logoColor=black) |
| ë°±ì—”ë“œ í”„ë ˆì„ì›Œí¬    | ![Spring Boot](https://img.shields.io/badge/Spring%20Boot-6DB33F?style=flat&logo=springboot&logoColor=white)&nbsp;![FastAPI](https://img.shields.io/badge/FastAPI-009688?style=flat&logo=fastapi&logoColor=white)&nbsp;![Flask](https://img.shields.io/badge/Flask-000000?style=flat&logo=flask&logoColor=white) |
| í”„ë¡ íŠ¸ì—”ë“œ           | ![React](https://img.shields.io/badge/React-61DAFB?style=flat&logo=react&logoColor=white)&nbsp;![Vite](https://img.shields.io/badge/Vite-646CFF?style=flat&logo=vite&logoColor=white)&nbsp;![Axios](https://img.shields.io/badge/Axios-5A29E4?style=flat&logo=axios&logoColor=white)&nbsp;![Fetch API](https://img.shields.io/badge/Fetch%20API-FF4154?style=flat&logo=javascript&logoColor=white) |
| ORM / ë°ì´í„° ì ‘ê·¼     | ![MyBatis](https://img.shields.io/badge/MyBatis-000000?style=flat&logo=mybatis&logoColor=white)                                       |
| ë°ì´í„°ë² ì´ìŠ¤          | ![Redis](https://img.shields.io/badge/Redis-DC382D?style=flat&logo=redis&logoColor=white)&nbsp;![MongoDB Atlas](https://img.shields.io/badge/MongoDB%20Atlas-47A248?style=flat&logo=mongodb&logoColor=white)&nbsp;![Oracle](https://img.shields.io/badge/Oracle-F80000?style=flat&logo=oracle&logoColor=white) |
| ì¸ì¦ / ë³´ì•ˆ          | ![JWT](https://img.shields.io/badge/JWT-000000?style=flat&logo=jsonwebtokens&logoColor=white)&nbsp;![OAuth2](https://img.shields.io/badge/OAuth2-EB5424?style=flat&logo=open%20id&logoColor=white)&nbsp; |
| AI / ì™¸ë¶€ API        | ![OpenAI](https://img.shields.io/badge/OpenAI-412991?style=flat&logo=openai&logoColor=white)&nbsp;![Perplexity.ai](https://img.shields.io/badge/Perplexity.ai-000000?style=flat&logo=perplexity-ai&logoColor=white) ![Naver](https://img.shields.io/badge/Naver-03C75A?style=flat&logo=naver&logoColor=white)&nbsp;![Google](https://img.shields.io/badge/Google-EA4335?style=flat&logo=google&logoColor=white)&nbsp;![Kakao](https://img.shields.io/badge/Kakao-FFCD00?style=flat&logo=kakao&logoColor=black) |
| ë°°í¬ / í˜¸ìŠ¤íŒ…        | ![Render](https://img.shields.io/badge/Render-46E3B7?style=flat&logo=render&logoColor=black)                                          |
| ê°œë°œ ë„êµ¬ / IDE      | ![IntelliJ IDEA](https://img.shields.io/badge/IntelliJ%20IDEA-000000?style=flat&logo=intellijidea&logoColor=white)&nbsp;![STS](https://img.shields.io/badge/Spring%20Tool%20Suite-6DB33F?style=flat&logo=spring&logoColor=white)&nbsp;![VS Code](https://img.shields.io/badge/VS%20Code-007ACC?style=flat&logo=visualstudiocode&logoColor=white) |
| í˜•ìƒ ê´€ë¦¬ / í˜‘ì—…     | ![GitHub](https://img.shields.io/badge/GitHub-181717?style=flat&logo=github&logoColor=white)&nbsp;![Notion](https://img.shields.io/badge/Notion-000000?style=flat&logo=notion&logoColor=white) |

## ì£¼ìš” í¬ë¡¤ë§ ì½”ë“œ ğŸ•·ï¸

<details>
<summary><strong>ë„¤ì´ë²„ ì¦ê¶Œ KOSPI/KOSDAQ í¬ë¡¤ë§ ì½”ë“œ</strong></summary>

**íŒŒì¼ëª…**: `crawler_krx_naver.py`  
**ìš©ë„**: ë„¤ì´ë²„ ì¦ê¶Œì—ì„œ KOSPI/KOSDAQ ì „ ì¢…ëª© ì‹œì„¸ë¥¼ ë§¤ì¼ ìë™ í¬ë¡¤ë§ â†’ MongoDB ì €ì¥ + Redis ìºì‹œ ê°±ì‹   
**ìë™í™”**: Linux(Ubuntu) crontabì„ í™œìš©í•œ ì›”~ê¸ˆ 09ì‹œë¶€í„° 15ì‹œ30ë¶„ê¹Œì§€ 10ë¶„ ê°„ê²© ì‹¤í–‰ ì˜ˆì•½
    
```python
# crawler_krx_naver.py
import requests
from bs4 import BeautifulSoup
import pymongo
import redis
from datetime import datetime
import time
import random
from pymongo import UpdateOne
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry
from zoneinfo import ZoneInfo

# ================== MongoDB + Redis ì—°ê²° ==================
mongo_client = pymongo.MongoClient("mongodb+srv://kh:1234@cluster0.fbav0ho.mongodb.net/")
db = mongo_client["stock"]
kospi_col = db["naver_kospi"]      # KOSPI ì»¬ë ‰ì…˜
kosdaq_col = db["naver_kosdaq"]    # KOSDAQ ì»¬ë ‰ì…˜

# Redis ì—°ê²° (ìºì‹œ ë¬´íš¨í™”ìš©)
try:
    r = redis.Redis(host='localhost', port=6379, db=0, decode_responses=True)
    redis_connected = True
    print("Redis ì—°ê²° ì„±ê³µ")
except Exception:
    r = None
    redis_connected = False
    print("Redis ë¯¸ì—°ê²° â†’ ìºì‹œ ê°±ì‹  ìƒëµ")

# MongoDB code ê¸°ì¤€ unique ì¸ë±ìŠ¤ ìë™ ìƒì„±
for col, name in [(kospi_col, "naver_kospi"), (kosdaq_col, "naver_kosdaq")]:
    if "code_1" not in col.index_information():
        col.create_index("code", unique=True, name="code_1")
        print(f"[{name}] code ì¸ë±ìŠ¤ ìƒì„±")
    else:
        print(f"[{name}] code ì¸ë±ìŠ¤ ì´ë¯¸ ì¡´ì¬")

# ================== Requests Session ì„¤ì • ==================
session = requests.Session()
session.headers.update({
    "User-Agent": random.choice([
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 Chrome/123.0 Safari/537.36",
        "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 Safari/605.1.15",
        "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 Chrome/123.0 Safari/537.36"
    ]),
    "Referer": "https://finance.naver.com/",
    "Accept-Language": "ko-KR,ko;q=0.9"
})

# ë„¤íŠ¸ì›Œí¬ ì˜¤ë¥˜ ì¬ì‹œë„ ì„¤ì •
retries = Retry(total=5, backoff_factor=1, status_forcelist=[500, 502, 503, 504, 429])
session.mount("https://", HTTPAdapter(max_retries=retries))

# ================== ë°ì´í„° ì •ì œ í•¨ìˆ˜ ==================
def clean_int(text):
    if not text or text.strip() in ["N/A", "-", ""]:
        return None
    return int(text.replace(",", ""))

def clean_float(text):
    if not text or text.strip() in ["N/A", "-", ""]:
        return None
    return float(text.replace(",", ""))

# ================== ë‹¨ì¼ í˜ì´ì§€ í¬ë¡¤ë§ ==================
def crawl_page(sosok, page):
    url = f"https://finance.naver.com/sise/sise_market_sum.naver?sosok={sosok}&page={page}"
    try:
        res = session.get(url, timeout=12)
        res.raise_for_status()
        soup = BeautifulSoup(res.text, "lxml")
        
        rows = soup.select("table.type_2 tbody tr[onmouseover]")
        data = []
        today = datetime.now(ZoneInfo("Asia/Seoul")).strftime("%Y-%m-%d")
        
        for row in rows:
            cols = row.find_all("td")
            if len(cols) < 12: continue
            a_tag = cols[1].find("a")
            if not a_tag: continue
                
            code = a_tag["href"].split("code=")[-1]
            data.append({
                "rank": clean_int(cols[0].get_text(strip=True)),
                "name": a_tag.get_text(strip=True),
                "code": code,
                "current_price": clean_int(cols[2].get_text(strip=True)),
                "change": cols[3].get_text(strip=True),
                "change_rate": cols[4].get_text(strip=True),
                "face_value": clean_int(cols[5].get_text(strip=True)),
                "market_cap": clean_int(cols[6].get_text(strip=True)),
                "listed_shares": clean_int(cols[7].get_text(strip=True)),
                "foreign_ratio": clean_float(cols[8].get_text(strip=True)),
                "volume": clean_int(cols[9].get_text(strip=True)),
                "per": clean_float(cols[10].get_text(strip=True)),
                "roe": clean_float(cols[11].get_text(strip=True)),
                "crawl_date": today,
                "crawled_at": datetime.now(ZoneInfo("Asia/Seoul")),
                "market": "KOSPI" if sosok == 0 else "KOSDAQ"
            })
        return data
    except Exception as e:
        print(f"[{'KOSPI' if sosok==0 else 'KOSDAQ'} {page}p] ì˜¤ë¥˜: {e}")
        return []

# ================== ë©”ì¸ í¬ë¡¤ë§ ì‹¤í–‰ ==================
def run_crawler():
    total_items = 0
    for market_name, sosok, collection in [
        ("KOSPI", 0, kospi_col),
        ("KOSDAQ", 1, kosdaq_col)
    ]:
        print(f"\n{market_name} í¬ë¡¤ë§ ì‹œì‘...")
        all_items = []
        empty_streak = 0
        
        for page in range(1, 60):
            items = crawl_page(sosok, page)
            if not items:
                empty_streak += 1
                if empty_streak >= 3:
                    print(f"{market_name} ë¹ˆ í˜ì´ì§€ ì—°ì† â†’ ì¢…ë£Œ")
                    break
            else:
                empty_streak = 0
                all_items.extend(items)
            print(f" {page:2d}í˜ì´ì§€ â†’ {len(items):3d}ê°œ")
            time.sleep(random.uniform(0.3, 0.7))
        
        if all_items:
            ops = [UpdateOne({"code": x["code"]}, {"$set": x}, upsert=True) for x in all_items]
            result = collection.bulk_write(ops, ordered=False)
            print(f"{market_name} ì €ì¥ ì™„ë£Œ â†’ ì‚½ì… {result.upserted_count}, ìˆ˜ì • {result.modified_count}")
            total_items += len(all_items)
    
    # Redis ìºì‹œ ë¬´íš¨í™”
    if redis_connected and r:
        deleted = r.delete("krx_kospi_list", "krx_kosdaq_list")
        print(f"Redis ìºì‹œ ê°±ì‹  ì™„ë£Œ (ì‚­ì œëœ í‚¤: {deleted}ê°œ)")
    
    print(f"\nì „ì²´ í¬ë¡¤ë§ ì™„ë£Œ! ì´ {total_items}ê°œ ì¢…ëª© ì—…ë°ì´íŠ¸")

if __name__ == "__main__":
    start_time = time.time()
    run_crawler()
    print(f"\nì†Œìš” ì‹œê°„: {time.time() - start_time:.1f}ì´ˆ")
```

<details>
<summary><strong>ë„¤ì´ë²„ êµ­ë‚´ ë‰´ìŠ¤ í¬ë¡¤ë§ ì½”ë“œ</strong></summary>
```python
import asyncio
import aiohttp
from bs4 import BeautifulSoup
from datetime import datetime
import os

from pymongo.mongo_client import MongoClient
from pymongo.server_api import ServerApi

# -------------------------
# MongoDB ì—°ê²°
# -------------------------
MONGO_URI = os.environ.get("MONGO_URI")

# ë¡œì»¬ í…ŒìŠ¤íŠ¸í•  ë•Œë§Œ ì•„ë˜ ì£¼ì„ í’€ì–´ì„œ ì‚¬ìš©í•˜ì„¸ìš”
# if not MONGO_URI:
#     MONGO_URI = "mongodb+srv://..." 

if not MONGO_URI:
    raise RuntimeError("MONGO_URI not set in crawler")

client = MongoClient(MONGO_URI, server_api=ServerApi("1"))
db = client["stock"]
collection = db["news_crawling"]

# -------------------------
# ë‰´ìŠ¤ ì¹´í…Œê³ ë¦¬ë³„ URL
# -------------------------
CATEGORY_URLS = {
    "ê¸ˆìœµ": "https://news.naver.com/breakingnews/section/101/259",
    "ì¦ê¶Œ": "https://news.naver.com/breakingnews/section/101/258",
    "ì‚°ì—…/ì¬ê³„": "https://news.naver.com/breakingnews/section/101/261",
    "ì¤‘ê¸°/ë²¤ì²˜": "https://news.naver.com/breakingnews/section/101/771",
    "ê¸€ë¡œë²Œ ê²½ì œ": "https://news.naver.com/breakingnews/section/101/260",
    "ìƒí™œê²½ì œ": "https://news.naver.com/breakingnews/section/101/310",
    "ê²½ì œ ì¼ë°˜": "https://news.naver.com/breakingnews/section/101/263",
}

HEADERS = {"User-Agent": "Mozilla/5.0"}

# -------------------------
# ë¡œê·¸ ì¶œë ¥
# -------------------------
def log(msg):
    print(f"[{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}] {msg}")

# -------------------------
# URL ë³€í™˜
# -------------------------
def to_pc_url(link):
    if "m.news.naver.com" in link:
        return link.replace("m.news.naver.com", "n.news.naver.com")
    return link

# -------------------------
# ë‰´ìŠ¤ ìƒì„¸ í¬ë¡¤ë§
# -------------------------
async def fetch_news_detail(session, link):
    link = to_pc_url(link)
    author = content = media = mediaLogo = image_url = pubDate = ""

    try:
        headers = HEADERS.copy()
        headers.update(
            {
                "Referer": "https://news.naver.com/",
                "Accept-Language": "ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7",
            }
        )

        async with session.get(link, headers=headers, timeout=15) as resp:
            html = await resp.text()
            soup = BeautifulSoup(html, "lxml")

            # ì‘ì„±ì
            author_tag = soup.select_one(
                ".byline span, .byline, .article_info, .writer"
            )
            if author_tag:
                author = author_tag.get_text(strip=True)

            # ë³¸ë¬¸
            content_tag = (
                soup.select_one("#articleBodyContents")
                or soup.select_one("#dic_area")
                or soup.select_one(".news_end")
                or soup.select_one(".article_body")
            )
            if content_tag:
                for s in content_tag.select(
                    "script, style, .ad, .link_area, iframe"
                ):
                    s.decompose()
                content = content_tag.get_text(separator="\n").strip()

            # ì–¸ë¡ ì‚¬
            meta_author = soup.select_one(
                "meta[property='og:article:author'], meta[name='author']"
            )
            if meta_author and meta_author.has_attr("content"):
                media = meta_author["content"].strip()

            # ëŒ€í‘œ ì´ë¯¸ì§€
            meta_image = soup.select_one("meta[property='og:image']")
            if meta_image and meta_image.has_attr("content"):
                image_url = meta_image["content"].strip()

            # ì‘ì„±ì¼
            meta_date = soup.select_one(
                'meta[property="article:published_time"]'
            )
            if meta_date and meta_date.has_attr("content"):
                pubDate = meta_date["content"].strip()
            else:
                date_tag = soup.select_one('span._ARTICLE_DATE_TIME')
                if date_tag and date_tag.has_attr("data-date-time"):
                    pubDate = date_tag["data-date-time"].strip()

            # ì–¸ë¡ ì‚¬ ë¡œê³ 
            def first_url_from_srcset(s):
                if not s:
                    return ""
                parts = s.split(",")
                first = parts[0].strip().split(" ")[0]
                return first

            logo_tag = soup.select_one("img.media_end_head_top_logo_img")
            if logo_tag:
                for a in (
                    "src",
                    "data-src",
                    "data-original",
                    "data-lazy-src",
                    "data-srcset",
                    "srcset",
                ):
                    if logo_tag.has_attr(a):
                        val = logo_tag.get(a, "").strip()
                        if a in ("srcset", "data-srcset"):
                            val = first_url_from_srcset(val)
                        if val:
                            mediaLogo = val
                            break

            if not mediaLogo:
                pc_logo = soup.select_one(".media_end_head_top_logo img")
                if pc_logo:
                    for a in ("src", "data-src", "srcset"):
                        if pc_logo.has_attr(a):
                            val = pc_logo.get(a, "").strip()
                            if a == "srcset":
                                val = first_url_from_srcset(val)
                            if val:
                                mediaLogo = val
                                break

            if not media:
                meta_site = soup.select_one(
                    "meta[property='og:site_name']"
                )
                if meta_site and meta_site.has_attr("content"):
                    media = meta_site["content"].strip()
            if media and media.endswith("| ë„¤ì´ë²„"):
                media = media.replace("| ë„¤ì´ë²„", "").strip()

    except Exception as e:
        log(f"âš  ë‰´ìŠ¤ ìƒì„¸ í¬ë¡¤ë§ ì‹¤íŒ¨: {link} / Error: {e}")

    return author, content, media, mediaLogo, image_url, pubDate

# -------------------------
# ë‰´ìŠ¤ ë¦¬ìŠ¤íŠ¸ í¬ë¡¤ë§
# -------------------------
async def fetch_news_list(session, url, max_items=1000):
    news_list = []
    try:
        async with session.get(url, headers=HEADERS, timeout=10) as resp:
            html = await resp.text()
            soup = BeautifulSoup(html, "lxml")
            items = soup.select("a.sa_text_title")

            for i, a in enumerate(items):
                if i >= max_items:
                    break
                href = a["href"]
                if href.startswith("/"):
                    href = "https://news.naver.com" + href
                title = a.get_text(strip=True)
                news_list.append({"link": href, "title": title})
    except Exception as e:
        log(f"âš  ë‰´ìŠ¤ ë¦¬ìŠ¤íŠ¸ í¬ë¡¤ë§ ì‹¤íŒ¨: {url} / Error: {e}")
    return news_list

# -------------------------
# ì¹´í…Œê³ ë¦¬ë³„ í¬ë¡¤ë§
# -------------------------
async def crawl_category(session, category, url):
    news_list = await fetch_news_list(session, url)
    tasks = []
    valid_news = []

    for news in news_list:
        if collection.find_one({"link": news["link"]}):
            log(f"[SKIP] ì´ë¯¸ ì €ì¥ë¨: {news['title']}")
            continue

        tasks.append(fetch_news_detail(session, news["link"]))
        valid_news.append(news)

        # ì„ì‹œ ë¬¸ì„œ ì‚½ì… (ìƒì„¸ í¬ë¡¤ í›„ í’ˆì§ˆê²€ì‚¬ì—ì„œ ê±¸ëŸ¬ì§ˆ ìˆ˜ ìˆìŒ)
        collection.update_one(
            {"link": news["link"]},
            {
                "$setOnInsert": {
                    "title": news["title"],
                    "link": news["link"],
                    "category": category,
                    "author": "",
                    "content": "",
                    "media": "",
                    "mediaLogo": "",
                    "image_url": "",
                    "pubDate": "",
                }
            },
            upsert=True,
        )

    results = await asyncio.gather(*tasks)

    for (author, content, media, mediaLogo, image_url, pubDate), news in zip(
        results, valid_news
    ):
        has_title = bool(news.get("title", "").strip())
        has_content = bool(content and content.strip())
        has_media = bool(media and media.strip())
        has_date = bool(pubDate and pubDate.strip())

        # ì œëª©ì´ ì—†ê±°ë‚˜, (ë³¸ë¬¸ë„ ì—†ê³  ì–¸ë¡ ì‚¬/ë‚ ì§œë„ ì—†ìœ¼ë©´) ì‚­ì œ
        if not has_title or (not has_content and not (has_media and has_date)):
            log(f"[DROP] ë‚´ìš© ë¶€ì¡±ìœ¼ë¡œ ì‚­ì œ: {news['title']}")
            collection.delete_one({"link": news["link"]})
            continue

        # pubDateê°€ ë¹„ì–´ ìˆìœ¼ë©´ ë‚ ì§œ ì—†ëŠ” ê¸°ì‚¬ë¼ì„œ ì œê±°
        if not has_date:
            log(f"[DROP] ë‚ ì§œ ì—†ìŒìœ¼ë¡œ ì‚­ì œ: {news['title']}")
            collection.delete_one({"link": news["link"]})
            continue

        collection.update_one(
            {"link": news["link"]},
            {
                "$set": {
                    "author": author,
                    "content": content,
                    "media": media,
                    "mediaLogo": mediaLogo,
                    "image_url": image_url,
                    "pubDate": pubDate,
                }
            },
        )

    log(f"âœ… {category} ë‰´ìŠ¤ í¬ë¡¤ë§ ì™„ë£Œ. ì´ ì €ì¥: {len(valid_news)}ê±´")

    # -------------------------
    # [ìˆ˜ì •ë¨] ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜
    # ì´ë¦„ ë³€ê²½: main -> task_korea_crawling
    # -------------------------
    async def task_korea_crawling():
        async with aiohttp.ClientSession() as session:
            for category, url in CATEGORY_URLS.items():
                log(f"=== ğŸ‡°ğŸ‡· êµ­ë‚´ ë‰´ìŠ¤ í¬ë¡¤ë§ ì‹œì‘: {category} ===")
                await crawl_category(session, category, url)
            log("ğŸ‰ êµ­ë‚´ ë‰´ìŠ¤ í¬ë¡¤ë§ ì „ì²´ ì™„ë£Œ!")
    
    # ì›ë˜ ìˆë˜ ë¬´í•œë£¨í”„(periodic_crawl)ì™€ ì‹¤í–‰ë¶€(__name__)ëŠ” ì‚­ì œí–ˆìŠµë‹ˆë‹¤.
    # app.pyì—ì„œ task_korea_crawling í•¨ìˆ˜ë§Œ importí•´ì„œ ì‚¬ìš©í•©ë‹ˆë‹¤.

```
